{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import convert_to_tensor\n",
    "\n",
    "VERBOSE = 1  # Set verbose to 1 to see training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=False, input_shape=(None, 1))) # This adds an LSTM layer with 50 neurons (units)\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use preprocessed dataset with only LCLid (unique consumer ID) and KWH/hh (per hour) (energy consumption per hour) columns replace the following cell with the code snipped below:\n",
    "\n",
    "hourly_data = pd.read_csv(\"Preprocessed_data2013_2.csv\", dtype={'LCLid': np.int16, 'KWH/hh (per hour) ': np.float64})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data = pd.read_csv(\"Preprocessed_data.csv\", dtype={'LCLid': np.int16, 'KWH/hh (per hour) ': np.float64, 'dayoftheyear': np.int16,\n",
    "       'hour': np.int8, 'is_weekend': np.int8})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of meters to be used in the project, replace the  hourly_data['LCLid'].max() with another number (e.g. NUM_OF_METERS = 100) to reduce\n",
    "# number of meters used.\n",
    "NUM_OF_METERS = hourly_data['LCLid'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data into train, validation, and test sets\n",
    "def train_test_validate(data):\n",
    "\n",
    "    for i in range(0, NUM_OF_METERS):\n",
    "\n",
    "        \n",
    "        tmp_data = data[data['LCLid'] == i] # Get the data for current meter\n",
    "\n",
    "        \n",
    "        val_split = int(len(tmp_data) * 0.8)\n",
    "        test_split = int(len(tmp_data) * 0.9)\n",
    "\n",
    "        # Set initial splits for current meter\n",
    "        train_ = tmp_data[:val_split]\n",
    "        vali_ = tmp_data[val_split:test_split]\n",
    "        test_ = tmp_data[test_split:]\n",
    "\n",
    "        # Concatanate the test data \n",
    "        if (i > 0):\n",
    "            train = pd.concat([train, train_], ignore_index=True)\n",
    "            valid = pd.concat([valid, vali_], ignore_index=True)\n",
    "            test = pd.concat([test, test_], ignore_index=True)\n",
    "        else:\n",
    "            train = train_\n",
    "            valid = vali_\n",
    "            test = test_\n",
    "\n",
    "    return {\"train\": train, \"test\": test, \"validation\": valid}\n",
    "\n",
    "dataset_splits = train_test_validate(hourly_data)\n",
    "\n",
    "# Convert data to tensors\n",
    "x_train_tensor = tf.convert_to_tensor(np.asarray(dataset_splits[\"train\"].drop(columns=['KWH/hh (per hour) '])))\n",
    "y_train_tensor = tf.convert_to_tensor(np.asarray(dataset_splits[\"train\"]['KWH/hh (per hour) ']))\n",
    "x_val_tensor = tf.convert_to_tensor(np.asarray(dataset_splits[\"validation\"].drop(columns=['KWH/hh (per hour) '])))\n",
    "y_val_tensor = tf.convert_to_tensor(np.asarray(dataset_splits[\"validation\"]['KWH/hh (per hour) ']))\n",
    "x_test_tensor = tf.convert_to_tensor(np.asarray(dataset_splits[\"test\"].drop(columns=['KWH/hh (per hour) '])))\n",
    "y_test_tensor = tf.convert_to_tensor(np.asarray(dataset_splits[\"test\"]['KWH/hh (per hour) ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "model = get_model()\n",
    "history = model.fit(x_train_tensor, y_train_tensor, validation_data=(x_val_tensor, y_val_tensor), epochs=10, batch_size=512, verbose=VERBOSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = model.evaluate(x_test_tensor, y_test_tensor, verbose=VERBOSE)\n",
    "print(f\"Test MSE: {results[0]}, Test MAE: {results[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "rounds = np.arange(0,len(history.history['loss']))\n",
    "loss = history.history['loss']\n",
    "acc = history.history['mean_absolute_error']\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True, \n",
    "                    subplot_titles=(\"MEAN ABSOLUTE ERROR\", \"MEAN SQUARED ERROR\"))\n",
    "\n",
    "# Add scatter plot for accuracy\n",
    "fig.add_trace(go.Scatter(x=rounds, y=acc, mode='markers', name='MAE'), row=1, col=1)\n",
    "# Add line plot for accuracy\n",
    "fig.add_trace(go.Scatter(x=rounds, y=acc, mode='lines', name='MAE Line'), row=1, col=1)\n",
    "\n",
    "# Add scatter plot for loss\n",
    "fig.add_trace(go.Scatter(x=rounds, y=loss, mode='markers', name='MSE'), row=2, col=1)\n",
    "# Add line plot for loss\n",
    "fig.add_trace(go.Scatter(x=rounds, y=loss, mode='lines', name='MSE Line'), row=2, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=800,  # Height of the figure\n",
    "    title_text=\"Centralized Model\",\n",
    ")\n",
    "\n",
    "# Update x-axis for all subplots\n",
    "fig.update_xaxes(title_text=\"Round\", row=2, col=1)\n",
    "# Update y-axis for each subplot\n",
    "fig.update_yaxes(title_text=\"MEAN ABSOLUTE ERROR\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"MEAN SQUARED ERROR\", row=2, col=1)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
